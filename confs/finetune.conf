general {
  base_exp_dir = ./exp/dtu/finetune/DTU/CASE_NAME
  recording = [
    ./,
    ./data
    ./ops
    ./models
    ./loss
  ]
}

dataset {
  name = DTU
  testpath = ./sample_data/

  # finetune dataset
  test_dir_comment = train
  test_scan_id = CASE_NAME
  test_n_views = 3
  test_img_wh = [800, 600]
  test_clip_wh = [0, 0]

  train_img_idx = []
  test_img_idx = []
}

train {
  learning_rate = 2e-4
  learning_rate_milestone = [100000, 150000, 200000]
  learning_rate_factor = 0.5
  end_iter = 10000
  save_freq = 1000
  val_freq = 1000
  val_mesh_freq = 1000
  report_freq = 100

  N_rays = 512

  igr_n_samples = 16384
  validate_resolution_level = 4
  anneal_start = 0
  anneal_end = 0
  anneal_start_lod1 = 0
  anneal_end_lod1 = 0

  use_white_bkgd = False

  # Loss
  sdf_igr_weight = 0.1
  sdf_sparse_weight = 0.02  # it seems that, 0.02 make variance_network has smaller variance
  sdf_decay_param = 100   # cannot be too large, which decide the tsdf range
  color_pixel_weight = 1.0
  color_patch_weight = 0.2
  mask_weight = 0.5
  patch_loss_type = ncc

  if_fix_lod0_networks = False

}

model {
  finetune_lod = 1
  h_patch_size = 3

  sdf_network_lod0 {
    lod = 0,
    ch_in = 56,  # the channel num of fused pyramid features
    voxel_size = 0.02105263,  # 0.02083333, should be 2/95
    vol_dims = [96, 96, 96],
    hidden_dim = 128,

    cost_type = variance_mean
    d_pyramid_feature_compress = 16,
    regnet_d_out = 16,  # default 8
    num_sdf_layers = 4,
    multires = 6
  }


  sdf_network_lod1 {
    lod = 1,
    ch_in = 56,  # the channel num of fused pyramid features
    voxel_size = 0.0104712, #0.01041667, should be 2/191
    vol_dims = [192, 192, 192],
    hidden_dim = 128,

    cost_type = variance_mean
    d_pyramid_feature_compress = 8,
    regnet_d_out = 8,  # default 8
    num_sdf_layers = 4,
    multires = 6
  }

  sdf_network_finetune {
    voxel_size = 0.0104712, #0.01041667, should be 2/191
    vol_dims = [192, 192, 192],
    hidden_dim = 128,
    regnet_d_out = 8,  # ! 16 for lod0,  8 for lod1
    num_sdf_layers = 4,
  }


  variance_network {
    init_val = 0.2
  }

  rendering_network {
    in_geometry_feat_ch = 16 # default 8
    in_rendering_feat_ch = 56
    anti_alias_pooling = True   # maybe False, be better
  }

  trainer {
    n_samples = 64
    n_importance = 64
    n_outside = 0  # 128 if render_outside_uniform_sampling
    perturb = 1.0
    alpha_type = div
  }
}
